"""
Implementation Agent for Cross-Database Orchestration

This module implements the execution agent that:
1. Executes query plans generated by the planning agent
2. Handles database operation execution with retries and error handling
3. Performs result aggregation with enhanced type coercion and joins
4. Provides observability and performance optimizations
"""

import logging
import json
import time
import asyncio
from typing import Dict, List, Any, Optional, Set, Tuple, Iterator
import uuid
from datetime import datetime

from ...llm.client import get_llm_client
from ..registry.integrations import registry_client
from ..adapters import mongo, postgres, qdrant, slack
from ..adapters.base import DBAdapter
from .result_aggregator import ResultAggregator, JoinType, AggregationFunction
from .plans.base import QueryPlan, Operation, OperationStatus

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImplementationAgent:
    """
    Agent responsible for executing cross-database query plans.
    
    This agent handles:
    1. Execution of operations in query plans with proper parallelism
    2. Result aggregation across heterogeneous databases
    3. Error handling, retries, and observability
    4. Performance optimizations like streaming and caching
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the implementation agent.
        
        Args:
            config: Optional configuration dictionary with settings for:
                - max_parallel_operations: Maximum number of parallel operations (default: 4)
                - operation_timeout_seconds: Timeout for individual operations (default: 60)
                - max_retry_attempts: Maximum retry attempts for failed operations (default: 3)
                - retry_delay_seconds: Delay between retry attempts (default: 1)
                - observability_enabled: Whether to collect execution metrics (default: True)
                - cache_enabled: Whether to enable result caching (default: False)
        """
        self.config = config or {}
        self.llm_client = get_llm_client()
        self.registry_client = registry_client
        
        # Configure execution settings
        self.max_parallel_operations = self.config.get("max_parallel_operations", 4)
        self.operation_timeout_seconds = self.config.get("operation_timeout_seconds", 60)
        self.max_retry_attempts = self.config.get("max_retry_attempts", 3)
        self.retry_delay_seconds = self.config.get("retry_delay_seconds", 1)
        
        # Performance and observability settings
        observability_enabled = self.config.get("observability_enabled", True)
        cache_enabled = self.config.get("cache_enabled", False)
        
        # Initialize result aggregator
        self.result_aggregator = ResultAggregator({
            "observability_enabled": observability_enabled,
            "cache_enabled": cache_enabled,
            "cache_ttl_seconds": self.config.get("cache_ttl_seconds", 300),
            "max_retry_attempts": self.max_retry_attempts
        })
        
        # Cache for database adapters to avoid recreating them
        self._adapter_cache = {}
        
        # Execution metrics
        self.metrics = {
            "operation_timings": {},
            "retry_counts": {},
            "error_counts": {},
            "execution_start": None,
            "execution_end": None,
            "total_operations": 0,
            "failed_operations": 0,
            "successful_operations": 0
        }
    
    def get_adapter(self, source_type: str, connection_info: Dict[str, Any]) -> DBAdapter:
        """
        Get the appropriate adapter based on the source type.
        
        Args:
            source_type: Type of data source (postgres, mongodb, qdrant, slack, etc.)
            connection_info: Connection information for the data source
            
        Returns:
            Instance of the appropriate adapter
        """
        if source_type.lower() == "postgres":
            return postgres.PostgresAdapter(connection_info)
        elif source_type.lower() in ("mongodb", "mongo"):
            return mongo.MongoAdapter(connection_info)
        elif source_type.lower() == "qdrant":
            return qdrant.QdrantAdapter(connection_info)
        elif source_type.lower() == "slack":
            return slack.SlackAdapter(connection_info)
        else:
            raise ValueError(f"Unsupported data source type: {source_type}")
    
    async def _get_adapter(self, source_id: str) -> DBAdapter:
        """
        Get or create a database adapter for the specified source.
        
        Args:
            source_id: ID of the data source in the registry
            
        Returns:
            Database adapter for the source
        """
        # Check cache first
        if source_id in self._adapter_cache:
            return self._adapter_cache[source_id]
            
        # Get source info from registry
        source_info = self.registry_client.get_source_by_id(source_id)
        if not source_info:
            raise ValueError(f"Source {source_id} not found in registry")
            
        # Create adapter
        source_type = source_info.get("type")
        connection_info = source_info.get("connection_info", {})
        
        adapter = self.get_adapter(source_type, connection_info)
        
        # Cache adapter for reuse
        self._adapter_cache[source_id] = adapter
        
        return adapter
    
    async def _execute_operation(
        self, 
        operation: Operation,
        semaphore: asyncio.Semaphore,
        retry_attempt: int = 0
    ) -> Dict[str, Any]:
        """
        Execute a single operation with error handling and retries.
        
        Args:
            operation: Operation to execute
            semaphore: Semaphore for controlling parallel execution
            retry_attempt: Current retry attempt number
            
        Returns:
            Operation result
        """
        operation_start = time.time()
        
        try:
            # Run DRY only if specified in the operation
            dry_run = operation.metadata.get("dry_run", False)
            
            # Use semaphore to limit concurrency
            async with semaphore:
                # Update operation status
                operation.status = OperationStatus.RUNNING
                operation.metadata["start_time"] = datetime.now().isoformat()
                
                # Set timeout for the operation
                # For operations without adapters, we rely on their own timeout mechanisms
                if operation.operation_type not in ["combine", "transform"]:
                    adapter = await self._get_adapter(operation.source_id)
                    
                    # Execute operation with timeout
                    try:
                        if operation.operation_type == "query":
                            if dry_run:
                                # For dry run, just validate the query
                                result = await asyncio.wait_for(
                                    adapter.validate_query(operation.parameters.get("query")),
                                    timeout=self.operation_timeout_seconds
                                )
                            else:
                                # Execute the query
                                try:
                                    # Try execute_query first (backward compatibility)
                                    if hasattr(adapter, 'execute_query'):
                                        result = await asyncio.wait_for(
                                            adapter.execute_query(operation.parameters.get("query")),
                                            timeout=self.operation_timeout_seconds
                                        )
                                    else:
                                        # Fall back to execute
                                        result = await asyncio.wait_for(
                                            adapter.execute(operation.parameters.get("query")),
                                            timeout=self.operation_timeout_seconds
                                        )
                                except asyncio.TimeoutError:
                                    raise TimeoutError(f"Query execution timed out after {self.operation_timeout_seconds}s")
                        elif operation.operation_type == "count":
                            result = await asyncio.wait_for(
                                adapter.count_records(
                                    operation.parameters.get("table"),
                                    operation.parameters.get("filter")
                                ),
                                timeout=self.operation_timeout_seconds
                            )
                        elif operation.operation_type == "aggregate":
                            result = await asyncio.wait_for(
                                adapter.aggregate(
                                    operation.parameters.get("table"),
                                    operation.parameters.get("aggregations"),
                                    operation.parameters.get("filter")
                                ),
                                timeout=self.operation_timeout_seconds
                            )
                        elif operation.operation_type == "search":
                            result = await asyncio.wait_for(
                                adapter.search(
                                    operation.parameters.get("table"),
                                    operation.parameters.get("query"),
                                    operation.parameters.get("fields")
                                ),
                                timeout=self.operation_timeout_seconds
                            )
                        else:
                            logger.warning(f"Unsupported operation type: {operation.operation_type}")
                            result = {"error": f"Unsupported operation type: {operation.operation_type}"}
                    except asyncio.TimeoutError:
                        # Handle timeout
                        logger.error(f"Operation {operation.operation_id} timed out after {self.operation_timeout_seconds}s")
                        raise TimeoutError(f"Operation timed out after {self.operation_timeout_seconds}s")
                else:
                    # Handle transform and combine operations differently
                    # These typically don't involve database adapters
                    if operation.operation_type == "transform":
                        # Get transform function from parameters
                        transform_function = operation.parameters.get("transform_function")
                        if not transform_function:
                            raise ValueError("No transform function specified")
                            
                        # Apply transformation
                        # This is a simplified version - in practice, you might use a more sophisticated approach
                        input_data = operation.parameters.get("input_data", {})
                        
                        # Use LLM for transformation if specified
                        if operation.parameters.get("use_llm", False):
                            prompt = self.llm_client.render_template(
                                "data_transform.tpl",
                                operation=operation.to_dict(),
                                input_data=input_data
                            )
                            
                            llm_response = await self.llm_client.client.chat.completions.create(
                                model=self.llm_client.model_name,
                                messages=[{"role": "user", "content": prompt}],
                                temperature=0.2
                            )
                            
                            content = llm_response.choices[0].message.content
                            # Extract JSON string from content
                            if "```json" in content:
                                json_str = content.split("```json")[1].split("```")[0].strip()
                            else:
                                json_str = content.strip()
                                
                            result = json.loads(json_str)
                        else:
                            # Execute simple transformation
                            result = {"transformed_data": input_data}
                    elif operation.operation_type == "combine":
                        # This is a placeholder - actual implementation would depend on how data is combined
                        result = {"combined_data": "This is a placeholder for combined data"}
                    else:
                        result = {"error": f"Unsupported operation type: {operation.operation_type}"}
                
                # Update operation metadata
                operation_end = time.time()
                operation.metadata["end_time"] = datetime.now().isoformat()
                operation.metadata["duration_seconds"] = operation_end - operation_start
                
                # Record metrics
                self.metrics["operation_timings"][operation.operation_id] = operation_end - operation_start
                
                # Update operation status
                operation.status = OperationStatus.COMPLETED
                
                return {
                    "operation_id": operation.operation_id,
                    "success": True,
                    "result": result,
                    "duration_seconds": operation_end - operation_start
                }
                
        except Exception as e:
            # Handle retry logic
            if retry_attempt < self.max_retry_attempts and self._should_retry(e):
                # Update retry metrics
                self.metrics["retry_counts"][operation.operation_id] = retry_attempt + 1
                
                # Wait before retrying
                await asyncio.sleep(self.retry_delay_seconds * (2 ** retry_attempt))  # Exponential backoff
                
                # Retry operation
                logger.info(f"Retrying operation {operation.operation_id} (attempt {retry_attempt + 1})")
                return await self._execute_operation(operation, semaphore, retry_attempt + 1)
            
            # If retries exhausted or error is not retryable, record failure
            operation_end = time.time()
            operation.status = OperationStatus.FAILED
            operation.metadata["end_time"] = datetime.now().isoformat()
            operation.metadata["duration_seconds"] = operation_end - operation_start
            operation.metadata["error"] = str(e)
            
            # Record error metrics
            error_type = type(e).__name__
            if error_type not in self.metrics["error_counts"]:
                self.metrics["error_counts"][error_type] = 0
            self.metrics["error_counts"][error_type] += 1
            
            logger.error(f"Operation {operation.operation_id} failed: {e}")
            
            return {
                "operation_id": operation.operation_id,
                "success": False,
                "error": str(e),
                "duration_seconds": operation_end - operation_start
            }
    
    def _should_retry(self, error: Exception) -> bool:
        """
        Determine if an operation should be retried based on the error.
        
        Args:
            error: The error that occurred
            
        Returns:
            True if the operation should be retried, False otherwise
        """
        # Retry on connection errors, timeouts, and transient failures
        retryable_errors = (
            TimeoutError,
            ConnectionError,
            ConnectionRefusedError,
            ConnectionResetError
        )
        
        return isinstance(error, retryable_errors)
    
    async def _resolve_dependencies(
        self, 
        query_plan: QueryPlan,
        execution_results: Dict[str, Any]
    ) -> Set[str]:
        """
        Resolve dependencies and find operations that are ready to execute.
        
        Args:
            query_plan: The query plan being executed
            execution_results: Current execution results
            
        Returns:
            Set of operation IDs that are ready to execute
        """
        ready_ops = set()
        
        for op in query_plan.operations:
            # Skip operations that are already completed or running
            if op.status in [OperationStatus.COMPLETED, OperationStatus.RUNNING, OperationStatus.FAILED]:
                continue
                
            # Check if all dependencies are satisfied
            all_deps_satisfied = True
            for dep_id in op.depends_on:
                # Check if dependency exists in results and was successful
                if dep_id not in execution_results or not execution_results[dep_id].get("success", False):
                    all_deps_satisfied = False
                    break
            
            if all_deps_satisfied:
                ready_ops.add(op.operation_id)
        
        return ready_ops
    
    async def execute_plan(
        self, 
        query_plan: QueryPlan,
        user_question: str,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a query plan and aggregate results.
        
        Args:
            query_plan: The query plan to execute
            user_question: Original user question for context
            dry_run: Whether to perform a dry run without executing operations
            
        Returns:
            Execution results with aggregated data
        """
        # Record start time
        start_time = time.time()
        self.metrics["execution_start"] = datetime.now().isoformat()
        self.metrics["total_operations"] = len(query_plan.operations)
        
        # Create semaphore for controlling parallel execution
        semaphore = asyncio.Semaphore(self.max_parallel_operations)
        
        # Track execution results
        execution_results = {}
        
        # Flag for dry run
        if dry_run:
            for op in query_plan.operations:
                op.metadata["dry_run"] = True
        
        # Execute operations in dependency order
        while True:
            # Find operations that are ready to execute
            ready_ops = await self._resolve_dependencies(query_plan, execution_results)
            
            if not ready_ops:
                # Check if all operations are completed or failed
                all_done = all(op.status in [OperationStatus.COMPLETED, OperationStatus.FAILED] 
                              for op in query_plan.operations)
                if all_done:
                    break
                    
                # If no operations are ready but not all are done, wait a bit
                # This could happen if operations are still running
                await asyncio.sleep(0.1)
                continue
            
            # Create tasks for ready operations
            tasks = []
            for op_id in ready_ops:
                # Find operation in the plan
                op = next((op for op in query_plan.operations if op.operation_id == op_id), None)
                if op:
                    # Create task for executing operation
                    tasks.append(self._execute_operation(op, semaphore))
            
            # Execute operations in parallel
            if tasks:
                op_results = await asyncio.gather(*tasks)
                
                # Update execution results
                for result in op_results:
                    execution_results[result["operation_id"]] = result
        
        # Record completion time
        end_time = time.time()
        self.metrics["execution_end"] = datetime.now().isoformat()
        
        # Count successful and failed operations
        self.metrics["successful_operations"] = sum(1 for r in execution_results.values() if r.get("success", True))
        self.metrics["failed_operations"] = len(execution_results) - self.metrics["successful_operations"]
        
        # Extract operation results for aggregation
        operation_results = {op_id: result.get("result", {}) for op_id, result in execution_results.items()}
        
        # Determine if we need to aggregate results
        if query_plan.output_operation_id:
            # If plan specifies a specific output operation, use that result
            final_result = operation_results.get(query_plan.output_operation_id, {})
        else:
            # Otherwise, aggregate all results
            logger.info("Aggregating results from all operations")
            
            try:
                # Check if plan has aggregation metadata
                aggregation_config = query_plan.metadata.get("aggregation", {})
                
                # Use LLM aggregation by default for complex query plans
                if len(query_plan.operations) > 1 and len(execution_results) > 0:
                    final_result = await self.result_aggregator.aggregate_results(
                        query_plan, operation_results, user_question
                    )
                else:
                    # For single operation plans, just return that operation's result
                    final_result = next(iter(operation_results.values())) if operation_results else {}
            except Exception as e:
                logger.error(f"Error during result aggregation: {e}")
                final_result = {
                    "error": f"Failed to aggregate results: {str(e)}",
                    "partial_results": operation_results
                }
        
        # Create execution summary
        execution_summary = {
            "plan_id": query_plan.plan_id,
            "execution_time_seconds": end_time - start_time,
            "total_operations": len(query_plan.operations),
            "successful_operations": self.metrics["successful_operations"],
            "failed_operations": self.metrics["failed_operations"],
            "operation_details": {op.operation_id: {
                "status": op.status,
                "type": op.operation_type,
                "duration": op.metadata.get("duration_seconds"),
                "error": op.metadata.get("error")
            } for op in query_plan.operations},
            "metrics": {
                "operation_timings": self.metrics["operation_timings"],
                "error_types": self.metrics["error_counts"]
            }
        }
        
        # Return final result with execution summary
        return {
            "success": self.metrics["failed_operations"] == 0,
            "execution_summary": execution_summary,
            "result": final_result
        }
    
    def get_metrics(self) -> Dict[str, Any]:
        """
        Get collected metrics for the current instance.
        
        Returns:
            Dictionary of metrics
        """
        # Return a copy of metrics to avoid external modification
        return dict(self.metrics)
    
    def clear_metrics(self):
        """Clear collected metrics"""
        self.metrics = {
            "operation_timings": {},
            "retry_counts": {},
            "error_counts": {},
            "execution_start": None,
            "execution_end": None,
            "total_operations": 0,
            "failed_operations": 0,
            "successful_operations": 0
        }
    
    async def close(self):
        """Close all adapter connections and clean up resources"""
        for adapter in self._adapter_cache.values():
            try:
                if hasattr(adapter, 'close'):
                    await adapter.close()
            except Exception as e:
                logger.warning(f"Error closing adapter: {e}")
        
        self._adapter_cache.clear() 