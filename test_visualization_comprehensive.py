#!/usr/bin/env python3
"""
Comprehensive Visualization Pipeline Test

This is the ONLY test for the visualization system following the singular-tests rule.
It covers the complete pipeline from data input to chart configuration output.
"""

import asyncio
import sys
import os
import logging
import pandas as pd
import time
from pathlib import Path

# Add the server directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "server"))

# Disable other loggers to focus on our pipeline
logging.getLogger('matplotlib').setLevel(logging.CRITICAL)
logging.getLogger('urllib3').setLevel(logging.CRITICAL)
logging.getLogger('requests').setLevel(logging.CRITICAL)

class MockLLMClient:
    """Mock LLM client for testing"""
    
    def __init__(self):
        self.provider = "mock"
        self.model = "test-model"
    
    async def generate(self, prompt: str) -> str:
        """Mock generate method"""
        return "Mock LLM analysis: This appears to be a dataset suitable for visualization."
    
    def render_template(self, template_name: str, **kwargs) -> str:
        return f"Mock template {template_name} with {kwargs}"

async def test_visualization_pipeline():
    """
    Comprehensive test of the entire visualization pipeline.
    This single test covers ALL visualization functionality.
    """
    print("üß™ COMPREHENSIVE VISUALIZATION PIPELINE TEST")
    print("=" * 60)
    
    test_session_id = f"test_{int(time.time())}"
    
    # Setup logging to capture all pipeline activity
    pipeline_logger = logging.getLogger('visualization_pipeline')
    pipeline_logger.setLevel(logging.DEBUG)
    
    # Remove existing handlers
    for handler in pipeline_logger.handlers[:]:
        pipeline_logger.removeHandler(handler)
    
    # Add fresh handler
    log_file = 'visualization_pipeline.log'
    if os.path.exists(log_file):
        os.remove(log_file)
    
    handler = logging.FileHandler(log_file)
    handler.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    pipeline_logger.addHandler(handler)
    
    try:
        # Step 1: Import all visualization modules
        print("\n1Ô∏è‚É£ Testing module imports...")
        from agent.visualization.types import (
            VisualizationDataset, DataAnalysisResult, UserPreferences,
            ChartSelection, ChartRecommendation, DatasetDimensionality,
            VariableClassification, PlotlyConfig
        )
        from agent.visualization.analyzer import DataAnalysisModule
        from agent.visualization.selector import ChartSelectionEngine
        print("‚úÖ All modules imported successfully")
        
        # Step 2: Create test datasets (multiple scenarios)
        print("\n2Ô∏è‚É£ Creating test datasets...")
        
        # Scenario A: Time series data
        time_series_data = pd.DataFrame({
            'date': pd.date_range('2024-01-01', periods=100, freq='D'),
            'revenue': [1000 + i * 10 for i in range(100)],
            'customers': [50 + i * 2 for i in range(100)]
        })
        
        # Scenario B: Categorical comparison data
        categorical_data = pd.DataFrame({
            'region': ['North', 'South', 'East', 'West'] * 25,
            'sales': [100 + i * 5 for i in range(100)],
        })
        
        # Scenario C: Correlation data
        correlation_data = pd.DataFrame({
            'x_value': [i + (i % 3) * 2 for i in range(50)],
            'y_value': [i * 2 + (i % 5) * 3 for i in range(50)],
            'z_value': [i ** 1.5 + (i % 2) * 10 for i in range(50)]
        })
        
        test_datasets = [
            ("time_series", time_series_data, "show revenue trends over time"),
            ("categorical", categorical_data, "compare sales by region"),
            ("correlation", correlation_data, "analyze relationship between x and y values")
        ]
        
        print(f"‚úÖ Created {len(test_datasets)} test scenarios")
        
        # Step 3: Initialize components
        print("\n3Ô∏è‚É£ Initializing visualization components...")
        mock_llm = MockLLMClient()
        analyzer = DataAnalysisModule(mock_llm)
        selector = ChartSelectionEngine(mock_llm)
        print("‚úÖ Components initialized")
        
        # Step 4: Test the complete pipeline for each scenario
        print("\n4Ô∏è‚É£ Testing complete pipeline for each scenario...")
        
        all_results = []
        
        for scenario_name, test_data, user_intent in test_datasets:
            print(f"\n   üìä Testing scenario: {scenario_name}")
            
            scenario_session = f"{test_session_id}_{scenario_name}"
            
            # Create dataset
            dataset = VisualizationDataset(
                data=test_data,
                columns=list(test_data.columns),
                metadata={"test_scenario": scenario_name},
                source_info={"origin": "test", "intent": user_intent}
            )
            
            # Analyze dataset
            print(f"   üîç Running data analysis...")
            analysis_result = await analyzer.analyze_dataset(dataset, user_intent, scenario_session)
            
            # Validate analysis
            assert isinstance(analysis_result, DataAnalysisResult)
            assert analysis_result.dataset_size == len(test_data)
            assert len(analysis_result.variable_types) > 0
            
            print(f"   ‚úÖ Analysis complete: {analysis_result.dataset_size} rows")
            
            # Select chart
            print(f"   üéØ Selecting optimal chart...")
            user_prefs = UserPreferences(
                preferred_style='modern',
                performance_priority='medium',
                interactivity_level='high'
            )
            
            chart_selection = await selector.select_optimal_chart(analysis_result, user_prefs, scenario_session)
            
            # Validate selection
            assert isinstance(chart_selection, ChartSelection)
            assert chart_selection.primary_chart.chart_type is not None
            assert 0 <= chart_selection.primary_chart.confidence_score <= 1
            
            print(f"   ‚úÖ Chart selected: {chart_selection.primary_chart.chart_type}")
            
            # Store results for final validation
            all_results.append({
                "scenario": scenario_name,
                "chart_type": chart_selection.primary_chart.chart_type,
                "confidence": chart_selection.primary_chart.confidence_score
            })
        
        # Step 5: Validate cross-scenario results
        print("\n5Ô∏è‚É£ Validating cross-scenario results...")
        
        # Ensure we got results for all scenarios
        assert len(all_results) == len(test_datasets), f"Expected {len(test_datasets)} results, got {len(all_results)}"
        
        # Ensure different scenarios produced different chart types (unless logically same)
        chart_types = [r["chart_type"] for r in all_results]
        print(f"   üìä Chart types selected: {chart_types}")
        
        # Ensure confidence scores are reasonable
        confidences = [r["confidence"] for r in all_results]
        avg_confidence = sum(confidences) / len(confidences)
        assert avg_confidence > 0.5, f"Average confidence too low: {avg_confidence:.2f}"
        print(f"   üìà Average confidence: {avg_confidence:.1%}")
        
        # Step 6: Test error handling
        print("\n6Ô∏è‚É£ Testing error handling...")
        
        # Test with empty dataset
        try:
            empty_dataset = VisualizationDataset(
                data=pd.DataFrame(),
                columns=[],
                metadata={},
                source_info={}
            )
            empty_analysis = await analyzer.analyze_dataset(empty_dataset, "test empty", f"{test_session_id}_empty")
            print("   ‚úÖ Empty dataset handled gracefully")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Empty dataset error (expected): {str(e)}")
        
        # Test with malformed data
        try:
            malformed_data = pd.DataFrame({'col1': [None, None, None]})
            malformed_dataset = VisualizationDataset(
                data=malformed_data,
                columns=['col1'],
                metadata={},
                source_info={}
            )
            malformed_analysis = await analyzer.analyze_dataset(malformed_dataset, "test malformed", f"{test_session_id}_malformed")
            print("   ‚úÖ Malformed dataset handled gracefully")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Malformed dataset error (handled): {str(e)}")
        
        # Step 7: Validate logging output
        print("\n7Ô∏è‚É£ Validating logging output...")
        
        # Ensure log file was created and has content
        if os.path.exists(log_file):
            with open(log_file, 'r') as f:
                log_contents = f.read()
            
            # Check for key log entries
            required_log_entries = [
                "Starting dataset analysis",
                "Statistical analysis complete",
                "Variable classification complete",
                "Dimensionality assessment complete",
                "Starting chart selection",
                "Chart selection completed"
            ]
            
            missing_entries = [entry for entry in required_log_entries if entry not in log_contents]
            
            if missing_entries:
                print(f"   ‚ö†Ô∏è Missing log entries: {missing_entries}")
            else:
                print("   ‚úÖ All required log entries found")
            
            log_lines = log_contents.split('\n')
            print(f"   üìù Total log lines: {len([l for l in log_lines if l.strip()])}")
        else:
            print("   ‚ö†Ô∏è Log file not created")
        
        # Step 8: Performance validation
        print("\n8Ô∏è‚É£ Performance validation...")
        
        # Test with larger dataset
        large_data = pd.DataFrame({
            'id': range(10000),
            'value': [i * 1.5 + (i % 100) for i in range(10000)],
            'category': [f"Cat_{i % 20}" for i in range(10000)]
        })
        
        large_dataset = VisualizationDataset(
            data=large_data,
            columns=list(large_data.columns),
            metadata={"performance_test": True},
            source_info={"size": "large"}
        )
        
        start_time = time.time()
        large_analysis = await analyzer.analyze_dataset(large_dataset, "performance test", f"{test_session_id}_perf")
        analysis_time = time.time() - start_time
        
        assert large_analysis.dataset_size == 10000
        print(f"   ‚ö° Large dataset analysis time: {analysis_time:.2f}s")
        
        if analysis_time > 5.0:
            print("   ‚ö†Ô∏è Analysis time seems high for 10k rows")
        else:
            print("   ‚úÖ Performance acceptable")
        
        # Final validation
        print("\n‚úÖ COMPREHENSIVE TEST COMPLETED SUCCESSFULLY")
        print("=" * 60)
        print("üéØ Summary:")
        print(f"   ‚Ä¢ Tested {len(test_datasets)} data scenarios")
        print(f"   ‚Ä¢ All modules imported and functional")
        print(f"   ‚Ä¢ Pipeline handles errors gracefully")
        print(f"   ‚Ä¢ Logging system working")
        print(f"   ‚Ä¢ Performance acceptable")
        print(f"   ‚Ä¢ Average confidence: {avg_confidence:.1%}")
        
        # Print final results table
        print("\nüìä Results by scenario:")
        for result in all_results:
            print(f"   {result['scenario']:12} | {result['chart_type']:15} | {result['confidence']:6.1%}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå COMPREHENSIVE TEST FAILED")
        print(f"Error: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # Clean up
        if os.path.exists(log_file):
            print(f"\nüìù Log file created: {log_file}")
            # Display last few log lines
            with open(log_file, 'r') as f:
                lines = f.readlines()
                print("   Last 5 log entries:")
                for line in lines[-5:]:
                    if line.strip():
                        print(f"   {line.strip()}")

if __name__ == "__main__":
    # Run the comprehensive test
    success = asyncio.run(test_visualization_pipeline())
    
    if success:
        print("\nüéâ ALL TESTS PASSED - Visualization pipeline is working correctly!")
        sys.exit(0)
    else:
        print("\nüí• TESTS FAILED - Check errors above")
        sys.exit(1) 